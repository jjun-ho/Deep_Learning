{"cells":[{"cell_type":"markdown","metadata":{"id":"9OA0f9oxYGYb"},"source":["# HW#1 Regularization\n","\n","안녕하세요, 광운대학교 로봇학부의 오정현 교수입니다. 본 자료는 딥러닝 실습 수업을 위해 제작된 것입니다.\n","\n","파이썬 문법\n","- 점프투파이썬(https://wikidocs.net/book/1) 참고\n","\n","이번 과제는 딥러닝의 일반화 성능을 높이기 위한 Regularization을 해보는 것입니다.이미지 분류에 여러 가지 Regularization 기법을 적용해 보도록 하겠습니다. 대표적인 Regularization 기법으로 Dropout, Data augmentation, Batch Normalization 등이 있습니다.\n","\n","이번 과제는 (https://www.tensorflow.org/tutorials/keras/classification?hl=ko)를 참고하면 좋습니다."]},{"cell_type":"markdown","metadata":{"id":"L9KBzwzd7Bub"},"source":["#1. Data Generation\n","Data는 mnist dataset을 이용하도록 하겠습니다. mnist dataset은 원래 60000개의 training set이 주어져 있지만 overfitting을 유도하기 위하여 1000개의 data만 이용하려고 합니다. 1000개의 data로 이루어진 x_train과 y_train을 만들어 보세요. 그리고 2000개로 이루어진 large_x_train, large_y_train을 만들어보세요. 그리고 training data의 다른 범위에서 200개로 이루어진 x_validation과 y_validation도 만들어 보세요."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3K3NqD_wTyh","outputId":"bb7ca365-0550-44f8-fa44-754a85f95d66","executionInfo":{"status":"ok","timestamp":1679993197890,"user_tz":-540,"elapsed":6352,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 2s 0us/step\n","x_validation shape: (200, 28, 28)\n","y_validation shape: (200,)\n","x_train shape: (1000, 28, 28, 1)\n","y_train shape: (1000, 10)\n","large_x_train shape: (2000, 28, 28, 1)\n","large_y_train shape: (2000, 10)\n","x_validation shape: (200, 28, 28, 1)\n","y_validation shape: (200, 10)\n"]}],"source":["from __future__ import print_function\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, BatchNormalization, Activation, Flatten\n","from keras import backend as K\n","from matplotlib import pyplot\n","\n","batch_size = 28\n","num_classes = 10\n","epochs = 100\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(X_train, Y_train), (x_test,y_test) = mnist.load_data()\n","\n","### START CODE HERE ###\n","x_validation = X_train[:200]\n","y_validation = Y_train[:200]\n","large_x_train = X_train[200:2200]\n","large_y_train = Y_train[200:2200]\n","x_train = X_train[2200:3200]\n","y_train = Y_train[2200:3200]\n","### END CODE HERE ###\n","print(\"x_validation shape:\", x_validation.shape)\n","print(\"y_validation shape:\", y_validation.shape)\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    large_x_train = large_x_train.reshape(large_x_train.shape[0], 1, img_rows, img_cols)\n","    x_validation = x_validation.reshape(x_validation.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    large_x_train = large_x_train.reshape(large_x_train.shape[0], img_rows, img_cols, 1)\n","    x_validation = x_validation.reshape(x_validation.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","x_train = x_train.astype('float32')\n","large_x_train = large_x_train.astype('float32')\n","x_validation = x_validation.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","large_x_train /= 255\n","x_validation /= 255\n","x_test /= 255\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","large_y_train = keras.utils.to_categorical(large_y_train, num_classes)\n","y_validation = keras.utils.to_categorical(y_validation, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","assert large_x_train.shape[0]==2000\n","assert large_y_train.shape[0]==2000\n","assert x_train.shape[0]==1000\n","assert y_train.shape[0]==1000\n","assert x_validation.shape[0]==200\n","assert y_validation.shape[0]==200\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"large_x_train shape:\", large_x_train.shape)\n","print(\"large_y_train shape:\", large_y_train.shape)\n","print(\"x_validation shape:\", x_validation.shape)\n","print(\"y_validation shape:\", y_validation.shape)"]},{"cell_type":"markdown","metadata":{"id":"jlNTST2K_IRE"},"source":["#2. 모델 생성\n","복습 차원에서 MLP 분류모델을 만들어 보도록 하겠습니다. 모델의 마지막 레이어에는 활성화 함수로 10개의 출력을 가지는 softmax를 달겠습니다. 이를 통해서 모델은 이미지안의 숫자가 0부터 9까지의 숫자중에 어디에 가까운지를 확률적으로 나타냅니다. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"TYNI8KWo8PyC"},"source":["다음과 같은 MLP 모델을 만들어 보세요.\n","\n","| Layer (type) | Output Shape | Param # |\n","|------|------|------|\n","| Flatten | (None, 784) | 0 |\n","| Dense | (None, 1024) | 803840 |\n","| Dense | (None, 1024) | 1049600 |\n","| Dense | (None, 1024) | 1049600 |\n","| Flatten | (None, 1024) | 0 |\n","| Dense | (None, 10) | 10250 |"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"560L-utPx8z1","outputId":"1c262e9e-23e6-426b-922c-af335bceb380","executionInfo":{"status":"ok","timestamp":1679993201380,"user_tz":-540,"elapsed":3497,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," dense (Dense)               (None, 1024)              803840    \n","                                                                 \n"," dense_1 (Dense)             (None, 1024)              1049600   \n","                                                                 \n"," dense_2 (Dense)             (None, 1024)              1049600   \n","                                                                 \n"," flatten_1 (Flatten)         (None, 1024)              0         \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                10250     \n","                                                                 \n","=================================================================\n","Total params: 2,913,290\n","Trainable params: 2,913,290\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = Sequential()\n","### START CODE HERE ###\n","model.add(Flatten(input_shape=(28,28)))\n","model.add(Dense(1024))\n","model.add(Dense(1024))\n","model.add(Dense(1024))\n","model.add(Flatten())\n","model.add(Dense(10, activation='softmax'))\n","### END CODE HERE ###\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"azhIfmZeBZm8"},"source":["#3. Learning MLP\n","기본 MLP 분류모델을 학습해 보겠습니다. Overfitting은 Training data에 맞추어 과도하게 학습이 이루어져 Test data에서 높은 성능이 나지 않는 현상, 즉 Generalization 성능이 높지 않게 나타나는 현상을 의미합니다. 따라서 Overfitting이 발생하면 Training accuracy는 높지만 Test accuracy는 높지 않게 나타납니다. \n","\n","현재 모델이 overfitting이 발생하는지 체크해 보세요. 일부러 overfitting이 발생하도록 유도하였기 때문에 overfitting 현상이 나타나야 합니다."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1VoI2bCyBy-","outputId":"92da5c2f-2d56-4f44-9e14-d9e579d028d0","executionInfo":{"status":"ok","timestamp":1679993243401,"user_tz":-540,"elapsed":42032,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","36/36 [==============================] - 5s 30ms/step - loss: 2.4510 - accuracy: 0.0560 - val_loss: 2.3875 - val_accuracy: 0.0900\n","Epoch 2/100\n","36/36 [==============================] - 1s 16ms/step - loss: 2.3595 - accuracy: 0.0850 - val_loss: 2.3016 - val_accuracy: 0.1200\n","Epoch 3/100\n","36/36 [==============================] - 0s 11ms/step - loss: 2.2753 - accuracy: 0.1330 - val_loss: 2.2227 - val_accuracy: 0.1550\n","Epoch 4/100\n","36/36 [==============================] - 0s 10ms/step - loss: 2.1964 - accuracy: 0.1960 - val_loss: 2.1489 - val_accuracy: 0.2100\n","Epoch 5/100\n","36/36 [==============================] - 0s 7ms/step - loss: 2.1219 - accuracy: 0.2680 - val_loss: 2.0793 - val_accuracy: 0.3050\n","Epoch 6/100\n","36/36 [==============================] - 0s 8ms/step - loss: 2.0512 - accuracy: 0.3410 - val_loss: 2.0139 - val_accuracy: 0.3850\n","Epoch 7/100\n","36/36 [==============================] - 0s 7ms/step - loss: 1.9838 - accuracy: 0.4200 - val_loss: 1.9514 - val_accuracy: 0.4350\n","Epoch 8/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.9194 - accuracy: 0.4850 - val_loss: 1.8922 - val_accuracy: 0.5050\n","Epoch 9/100\n","36/36 [==============================] - 0s 9ms/step - loss: 1.8576 - accuracy: 0.5250 - val_loss: 1.8358 - val_accuracy: 0.5600\n","Epoch 10/100\n","36/36 [==============================] - 0s 10ms/step - loss: 1.7985 - accuracy: 0.5650 - val_loss: 1.7819 - val_accuracy: 0.5900\n","Epoch 11/100\n","36/36 [==============================] - 0s 9ms/step - loss: 1.7421 - accuracy: 0.5960 - val_loss: 1.7304 - val_accuracy: 0.6000\n","Epoch 12/100\n","36/36 [==============================] - 0s 10ms/step - loss: 1.6880 - accuracy: 0.6330 - val_loss: 1.6816 - val_accuracy: 0.6200\n","Epoch 13/100\n","36/36 [==============================] - 0s 9ms/step - loss: 1.6362 - accuracy: 0.6720 - val_loss: 1.6348 - val_accuracy: 0.6550\n","Epoch 14/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.5866 - accuracy: 0.6870 - val_loss: 1.5903 - val_accuracy: 0.6650\n","Epoch 15/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.5391 - accuracy: 0.7080 - val_loss: 1.5480 - val_accuracy: 0.6800\n","Epoch 16/100\n","36/36 [==============================] - 0s 9ms/step - loss: 1.4939 - accuracy: 0.7200 - val_loss: 1.5078 - val_accuracy: 0.7150\n","Epoch 17/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.4506 - accuracy: 0.7390 - val_loss: 1.4694 - val_accuracy: 0.7300\n","Epoch 18/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.4093 - accuracy: 0.7480 - val_loss: 1.4331 - val_accuracy: 0.7400\n","Epoch 19/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.3699 - accuracy: 0.7630 - val_loss: 1.3984 - val_accuracy: 0.7400\n","Epoch 20/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.3321 - accuracy: 0.7670 - val_loss: 1.3655 - val_accuracy: 0.7500\n","Epoch 21/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.2961 - accuracy: 0.7760 - val_loss: 1.3341 - val_accuracy: 0.7500\n","Epoch 22/100\n","36/36 [==============================] - 0s 9ms/step - loss: 1.2616 - accuracy: 0.7850 - val_loss: 1.3039 - val_accuracy: 0.7550\n","Epoch 23/100\n","36/36 [==============================] - 0s 8ms/step - loss: 1.2285 - accuracy: 0.7910 - val_loss: 1.2754 - val_accuracy: 0.7500\n","Epoch 24/100\n","36/36 [==============================] - 0s 9ms/step - loss: 1.1969 - accuracy: 0.7950 - val_loss: 1.2482 - val_accuracy: 0.7500\n","Epoch 25/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.1668 - accuracy: 0.7980 - val_loss: 1.2222 - val_accuracy: 0.7550\n","Epoch 26/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.1378 - accuracy: 0.8040 - val_loss: 1.1973 - val_accuracy: 0.7650\n","Epoch 27/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.1105 - accuracy: 0.8080 - val_loss: 1.1738 - val_accuracy: 0.7650\n","Epoch 28/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.0839 - accuracy: 0.8140 - val_loss: 1.1511 - val_accuracy: 0.7650\n","Epoch 29/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.0587 - accuracy: 0.8150 - val_loss: 1.1296 - val_accuracy: 0.7650\n","Epoch 30/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.0345 - accuracy: 0.8170 - val_loss: 1.1090 - val_accuracy: 0.7700\n","Epoch 31/100\n","36/36 [==============================] - 0s 5ms/step - loss: 1.0113 - accuracy: 0.8190 - val_loss: 1.0892 - val_accuracy: 0.7700\n","Epoch 32/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.9890 - accuracy: 0.8260 - val_loss: 1.0703 - val_accuracy: 0.7700\n","Epoch 33/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.9676 - accuracy: 0.8260 - val_loss: 1.0521 - val_accuracy: 0.7700\n","Epoch 34/100\n","36/36 [==============================] - 0s 4ms/step - loss: 0.9473 - accuracy: 0.8290 - val_loss: 1.0348 - val_accuracy: 0.7750\n","Epoch 35/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.9278 - accuracy: 0.8300 - val_loss: 1.0182 - val_accuracy: 0.7750\n","Epoch 36/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.9090 - accuracy: 0.8300 - val_loss: 1.0025 - val_accuracy: 0.7800\n","Epoch 37/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.8909 - accuracy: 0.8300 - val_loss: 0.9873 - val_accuracy: 0.7850\n","Epoch 38/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.8738 - accuracy: 0.8310 - val_loss: 0.9731 - val_accuracy: 0.7850\n","Epoch 39/100\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8571 - accuracy: 0.8320 - val_loss: 0.9592 - val_accuracy: 0.7850\n","Epoch 40/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.8412 - accuracy: 0.8340 - val_loss: 0.9458 - val_accuracy: 0.7850\n","Epoch 41/100\n","36/36 [==============================] - 0s 4ms/step - loss: 0.8258 - accuracy: 0.8380 - val_loss: 0.9329 - val_accuracy: 0.7850\n","Epoch 42/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.8109 - accuracy: 0.8410 - val_loss: 0.9207 - val_accuracy: 0.7850\n","Epoch 43/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.7967 - accuracy: 0.8400 - val_loss: 0.9088 - val_accuracy: 0.7850\n","Epoch 44/100\n","36/36 [==============================] - 0s 7ms/step - loss: 0.7829 - accuracy: 0.8430 - val_loss: 0.8974 - val_accuracy: 0.7850\n","Epoch 45/100\n","36/36 [==============================] - 0s 6ms/step - loss: 0.7697 - accuracy: 0.8500 - val_loss: 0.8863 - val_accuracy: 0.7850\n","Epoch 46/100\n","36/36 [==============================] - 0s 6ms/step - loss: 0.7569 - accuracy: 0.8480 - val_loss: 0.8758 - val_accuracy: 0.7850\n","Epoch 47/100\n","36/36 [==============================] - 0s 7ms/step - loss: 0.7447 - accuracy: 0.8540 - val_loss: 0.8656 - val_accuracy: 0.7900\n","Epoch 48/100\n","36/36 [==============================] - 0s 7ms/step - loss: 0.7329 - accuracy: 0.8530 - val_loss: 0.8558 - val_accuracy: 0.7950\n","Epoch 49/100\n","36/36 [==============================] - 0s 6ms/step - loss: 0.7216 - accuracy: 0.8560 - val_loss: 0.8467 - val_accuracy: 0.7950\n","Epoch 50/100\n","36/36 [==============================] - 0s 6ms/step - loss: 0.7107 - accuracy: 0.8550 - val_loss: 0.8377 - val_accuracy: 0.7950\n","Epoch 51/100\n","36/36 [==============================] - 0s 7ms/step - loss: 0.7001 - accuracy: 0.8580 - val_loss: 0.8289 - val_accuracy: 0.8000\n","Epoch 52/100\n","36/36 [==============================] - 0s 6ms/step - loss: 0.6898 - accuracy: 0.8590 - val_loss: 0.8205 - val_accuracy: 0.8000\n","Epoch 53/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6799 - accuracy: 0.8610 - val_loss: 0.8128 - val_accuracy: 0.8000\n","Epoch 54/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.8600 - val_loss: 0.8053 - val_accuracy: 0.8050\n","Epoch 55/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6610 - accuracy: 0.8650 - val_loss: 0.7977 - val_accuracy: 0.8050\n","Epoch 56/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6519 - accuracy: 0.8650 - val_loss: 0.7907 - val_accuracy: 0.8050\n","Epoch 57/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6432 - accuracy: 0.8680 - val_loss: 0.7836 - val_accuracy: 0.8050\n","Epoch 58/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6347 - accuracy: 0.8690 - val_loss: 0.7767 - val_accuracy: 0.8050\n","Epoch 59/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6265 - accuracy: 0.8700 - val_loss: 0.7700 - val_accuracy: 0.8050\n","Epoch 60/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6185 - accuracy: 0.8710 - val_loss: 0.7637 - val_accuracy: 0.8050\n","Epoch 61/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.6108 - accuracy: 0.8730 - val_loss: 0.7577 - val_accuracy: 0.8050\n","Epoch 62/100\n","36/36 [==============================] - 0s 6ms/step - loss: 0.6032 - accuracy: 0.8760 - val_loss: 0.7518 - val_accuracy: 0.8050\n","Epoch 63/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5960 - accuracy: 0.8730 - val_loss: 0.7461 - val_accuracy: 0.8050\n","Epoch 64/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5890 - accuracy: 0.8760 - val_loss: 0.7404 - val_accuracy: 0.8050\n","Epoch 65/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5819 - accuracy: 0.8800 - val_loss: 0.7351 - val_accuracy: 0.8050\n","Epoch 66/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5753 - accuracy: 0.8820 - val_loss: 0.7299 - val_accuracy: 0.8050\n","Epoch 67/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5687 - accuracy: 0.8810 - val_loss: 0.7248 - val_accuracy: 0.8100\n","Epoch 68/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5623 - accuracy: 0.8800 - val_loss: 0.7197 - val_accuracy: 0.8250\n","Epoch 69/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5562 - accuracy: 0.8850 - val_loss: 0.7148 - val_accuracy: 0.8250\n","Epoch 70/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5502 - accuracy: 0.8850 - val_loss: 0.7102 - val_accuracy: 0.8250\n","Epoch 71/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5443 - accuracy: 0.8870 - val_loss: 0.7058 - val_accuracy: 0.8250\n","Epoch 72/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5386 - accuracy: 0.8860 - val_loss: 0.7015 - val_accuracy: 0.8300\n","Epoch 73/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5330 - accuracy: 0.8890 - val_loss: 0.6975 - val_accuracy: 0.8300\n","Epoch 74/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5275 - accuracy: 0.8880 - val_loss: 0.6936 - val_accuracy: 0.8300\n","Epoch 75/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5222 - accuracy: 0.8870 - val_loss: 0.6895 - val_accuracy: 0.8300\n","Epoch 76/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5171 - accuracy: 0.8890 - val_loss: 0.6855 - val_accuracy: 0.8300\n","Epoch 77/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5119 - accuracy: 0.8930 - val_loss: 0.6815 - val_accuracy: 0.8300\n","Epoch 78/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5071 - accuracy: 0.8920 - val_loss: 0.6777 - val_accuracy: 0.8300\n","Epoch 79/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.5022 - accuracy: 0.8940 - val_loss: 0.6741 - val_accuracy: 0.8300\n","Epoch 80/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4976 - accuracy: 0.8940 - val_loss: 0.6704 - val_accuracy: 0.8300\n","Epoch 81/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4929 - accuracy: 0.8980 - val_loss: 0.6669 - val_accuracy: 0.8300\n","Epoch 82/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4886 - accuracy: 0.8950 - val_loss: 0.6636 - val_accuracy: 0.8300\n","Epoch 83/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.8970 - val_loss: 0.6606 - val_accuracy: 0.8300\n","Epoch 84/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.8990 - val_loss: 0.6574 - val_accuracy: 0.8300\n","Epoch 85/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4757 - accuracy: 0.8990 - val_loss: 0.6544 - val_accuracy: 0.8300\n","Epoch 86/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4716 - accuracy: 0.9000 - val_loss: 0.6517 - val_accuracy: 0.8300\n","Epoch 87/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4677 - accuracy: 0.9000 - val_loss: 0.6485 - val_accuracy: 0.8350\n","Epoch 88/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4638 - accuracy: 0.9010 - val_loss: 0.6455 - val_accuracy: 0.8350\n","Epoch 89/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4599 - accuracy: 0.9000 - val_loss: 0.6428 - val_accuracy: 0.8350\n","Epoch 90/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4561 - accuracy: 0.9030 - val_loss: 0.6400 - val_accuracy: 0.8350\n","Epoch 91/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4525 - accuracy: 0.9030 - val_loss: 0.6374 - val_accuracy: 0.8350\n","Epoch 92/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.9030 - val_loss: 0.6349 - val_accuracy: 0.8350\n","Epoch 93/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4452 - accuracy: 0.9040 - val_loss: 0.6322 - val_accuracy: 0.8350\n","Epoch 94/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.9040 - val_loss: 0.6298 - val_accuracy: 0.8400\n","Epoch 95/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4382 - accuracy: 0.9060 - val_loss: 0.6275 - val_accuracy: 0.8400\n","Epoch 96/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4349 - accuracy: 0.9070 - val_loss: 0.6251 - val_accuracy: 0.8400\n","Epoch 97/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4315 - accuracy: 0.9070 - val_loss: 0.6228 - val_accuracy: 0.8400\n","Epoch 98/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4283 - accuracy: 0.9070 - val_loss: 0.6208 - val_accuracy: 0.8450\n","Epoch 99/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4252 - accuracy: 0.9070 - val_loss: 0.6184 - val_accuracy: 0.8450\n","Epoch 100/100\n","36/36 [==============================] - 0s 5ms/step - loss: 0.4221 - accuracy: 0.9080 - val_loss: 0.6165 - val_accuracy: 0.8450\n","Test loss: 0.568427562713623\n","Test accuracy: 0.8517000079154968\n"]}],"source":["model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","weights = model.get_weights()\n","\n","history=model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_validation, y_validation))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{"id":"1jebNKzFBJGE"},"source":["#4. Regularization\n","Overfitting이 발생한 모델에 다양한 Regularization 기법을 이용해 보도록 합시다."]},{"cell_type":"markdown","metadata":{"id":"WsxZKsjCFvhT"},"source":["4.1 Large Dataset\n","\n","Training data가 충분하다면 overfitting 현상이 발생할 가능성이 줄어듭니다. 기본 MLP 분류 모델에서 large_x_train과 large_y_train을 이용하면 성능이 올라갈 것입니다. Generalization 성능이 올라갔나요?"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5b1fp96FuEy","outputId":"29aed632-529b-42b7-ea2d-bcebc87a935d","executionInfo":{"status":"ok","timestamp":1679993285810,"user_tz":-540,"elapsed":42424,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","72/72 [==============================] - 0s 4ms/step - loss: 2.3529 - accuracy: 0.1050 - val_loss: 2.2211 - val_accuracy: 0.1700\n","Epoch 2/100\n","72/72 [==============================] - 0s 4ms/step - loss: 2.1304 - accuracy: 0.2710 - val_loss: 2.0320 - val_accuracy: 0.3600\n","Epoch 3/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.9566 - accuracy: 0.4445 - val_loss: 1.8789 - val_accuracy: 0.5250\n","Epoch 4/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.8123 - accuracy: 0.5655 - val_loss: 1.7506 - val_accuracy: 0.6400\n","Epoch 5/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.6890 - accuracy: 0.6275 - val_loss: 1.6407 - val_accuracy: 0.6800\n","Epoch 6/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.5826 - accuracy: 0.6695 - val_loss: 1.5454 - val_accuracy: 0.7050\n","Epoch 7/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.4891 - accuracy: 0.7025 - val_loss: 1.4618 - val_accuracy: 0.7150\n","Epoch 8/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.4065 - accuracy: 0.7240 - val_loss: 1.3890 - val_accuracy: 0.7300\n","Epoch 9/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.3330 - accuracy: 0.7450 - val_loss: 1.3228 - val_accuracy: 0.7400\n","Epoch 10/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.2666 - accuracy: 0.7575 - val_loss: 1.2634 - val_accuracy: 0.7450\n","Epoch 11/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.2072 - accuracy: 0.7710 - val_loss: 1.2101 - val_accuracy: 0.7650\n","Epoch 12/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.1528 - accuracy: 0.7790 - val_loss: 1.1617 - val_accuracy: 0.7650\n","Epoch 13/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.1034 - accuracy: 0.7815 - val_loss: 1.1181 - val_accuracy: 0.7700\n","Epoch 14/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.0586 - accuracy: 0.7910 - val_loss: 1.0780 - val_accuracy: 0.7800\n","Epoch 15/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.0176 - accuracy: 0.7985 - val_loss: 1.0412 - val_accuracy: 0.7850\n","Epoch 16/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.9801 - accuracy: 0.8040 - val_loss: 1.0075 - val_accuracy: 0.7900\n","Epoch 17/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.9455 - accuracy: 0.8120 - val_loss: 0.9761 - val_accuracy: 0.7900\n","Epoch 18/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.9137 - accuracy: 0.8140 - val_loss: 0.9479 - val_accuracy: 0.8000\n","Epoch 19/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.8843 - accuracy: 0.8200 - val_loss: 0.9216 - val_accuracy: 0.8050\n","Epoch 20/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.8571 - accuracy: 0.8245 - val_loss: 0.8974 - val_accuracy: 0.8150\n","Epoch 21/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.8319 - accuracy: 0.8285 - val_loss: 0.8758 - val_accuracy: 0.8150\n","Epoch 22/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.8087 - accuracy: 0.8305 - val_loss: 0.8552 - val_accuracy: 0.8200\n","Epoch 23/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.7869 - accuracy: 0.8315 - val_loss: 0.8354 - val_accuracy: 0.8200\n","Epoch 24/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.7667 - accuracy: 0.8365 - val_loss: 0.8177 - val_accuracy: 0.8200\n","Epoch 25/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.7478 - accuracy: 0.8370 - val_loss: 0.8009 - val_accuracy: 0.8250\n","Epoch 26/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.7301 - accuracy: 0.8380 - val_loss: 0.7853 - val_accuracy: 0.8250\n","Epoch 27/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.7136 - accuracy: 0.8410 - val_loss: 0.7702 - val_accuracy: 0.8250\n","Epoch 28/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6979 - accuracy: 0.8430 - val_loss: 0.7565 - val_accuracy: 0.8250\n","Epoch 29/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.8440 - val_loss: 0.7440 - val_accuracy: 0.8300\n","Epoch 30/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6697 - accuracy: 0.8450 - val_loss: 0.7320 - val_accuracy: 0.8300\n","Epoch 31/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6568 - accuracy: 0.8490 - val_loss: 0.7203 - val_accuracy: 0.8300\n","Epoch 32/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6446 - accuracy: 0.8525 - val_loss: 0.7100 - val_accuracy: 0.8350\n","Epoch 33/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6329 - accuracy: 0.8560 - val_loss: 0.6998 - val_accuracy: 0.8500\n","Epoch 34/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.6221 - accuracy: 0.8565 - val_loss: 0.6897 - val_accuracy: 0.8500\n","Epoch 35/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.6116 - accuracy: 0.8620 - val_loss: 0.6807 - val_accuracy: 0.8500\n","Epoch 36/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6017 - accuracy: 0.8625 - val_loss: 0.6720 - val_accuracy: 0.8500\n","Epoch 37/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5923 - accuracy: 0.8645 - val_loss: 0.6646 - val_accuracy: 0.8500\n","Epoch 38/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5835 - accuracy: 0.8660 - val_loss: 0.6569 - val_accuracy: 0.8500\n","Epoch 39/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5750 - accuracy: 0.8665 - val_loss: 0.6494 - val_accuracy: 0.8550\n","Epoch 40/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.8675 - val_loss: 0.6427 - val_accuracy: 0.8550\n","Epoch 41/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5593 - accuracy: 0.8685 - val_loss: 0.6363 - val_accuracy: 0.8550\n","Epoch 42/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5519 - accuracy: 0.8685 - val_loss: 0.6303 - val_accuracy: 0.8550\n","Epoch 43/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5449 - accuracy: 0.8700 - val_loss: 0.6234 - val_accuracy: 0.8550\n","Epoch 44/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5381 - accuracy: 0.8725 - val_loss: 0.6180 - val_accuracy: 0.8550\n","Epoch 45/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5315 - accuracy: 0.8715 - val_loss: 0.6125 - val_accuracy: 0.8550\n","Epoch 46/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5253 - accuracy: 0.8765 - val_loss: 0.6070 - val_accuracy: 0.8550\n","Epoch 47/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5192 - accuracy: 0.8765 - val_loss: 0.6024 - val_accuracy: 0.8500\n","Epoch 48/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5134 - accuracy: 0.8775 - val_loss: 0.5973 - val_accuracy: 0.8500\n","Epoch 49/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5079 - accuracy: 0.8780 - val_loss: 0.5930 - val_accuracy: 0.8500\n","Epoch 50/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5024 - accuracy: 0.8785 - val_loss: 0.5887 - val_accuracy: 0.8500\n","Epoch 51/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4972 - accuracy: 0.8805 - val_loss: 0.5844 - val_accuracy: 0.8600\n","Epoch 52/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.8815 - val_loss: 0.5804 - val_accuracy: 0.8650\n","Epoch 53/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4873 - accuracy: 0.8830 - val_loss: 0.5764 - val_accuracy: 0.8600\n","Epoch 54/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4825 - accuracy: 0.8820 - val_loss: 0.5723 - val_accuracy: 0.8600\n","Epoch 55/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4780 - accuracy: 0.8825 - val_loss: 0.5683 - val_accuracy: 0.8600\n","Epoch 56/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4735 - accuracy: 0.8855 - val_loss: 0.5650 - val_accuracy: 0.8600\n","Epoch 57/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4692 - accuracy: 0.8880 - val_loss: 0.5616 - val_accuracy: 0.8600\n","Epoch 58/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4652 - accuracy: 0.8870 - val_loss: 0.5582 - val_accuracy: 0.8650\n","Epoch 59/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4612 - accuracy: 0.8895 - val_loss: 0.5552 - val_accuracy: 0.8650\n","Epoch 60/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.8895 - val_loss: 0.5521 - val_accuracy: 0.8650\n","Epoch 61/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4534 - accuracy: 0.8905 - val_loss: 0.5488 - val_accuracy: 0.8650\n","Epoch 62/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4497 - accuracy: 0.8920 - val_loss: 0.5461 - val_accuracy: 0.8650\n","Epoch 63/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4462 - accuracy: 0.8930 - val_loss: 0.5436 - val_accuracy: 0.8650\n","Epoch 64/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.8940 - val_loss: 0.5414 - val_accuracy: 0.8650\n","Epoch 65/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4393 - accuracy: 0.8935 - val_loss: 0.5387 - val_accuracy: 0.8700\n","Epoch 66/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4360 - accuracy: 0.8955 - val_loss: 0.5360 - val_accuracy: 0.8700\n","Epoch 67/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4327 - accuracy: 0.8960 - val_loss: 0.5335 - val_accuracy: 0.8700\n","Epoch 68/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4295 - accuracy: 0.8965 - val_loss: 0.5307 - val_accuracy: 0.8750\n","Epoch 69/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4265 - accuracy: 0.8960 - val_loss: 0.5286 - val_accuracy: 0.8750\n","Epoch 70/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4235 - accuracy: 0.8975 - val_loss: 0.5262 - val_accuracy: 0.8750\n","Epoch 71/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4205 - accuracy: 0.8985 - val_loss: 0.5241 - val_accuracy: 0.8750\n","Epoch 72/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4177 - accuracy: 0.8990 - val_loss: 0.5222 - val_accuracy: 0.8750\n","Epoch 73/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4149 - accuracy: 0.9000 - val_loss: 0.5199 - val_accuracy: 0.8750\n","Epoch 74/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4121 - accuracy: 0.8995 - val_loss: 0.5180 - val_accuracy: 0.8750\n","Epoch 75/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4094 - accuracy: 0.8990 - val_loss: 0.5168 - val_accuracy: 0.8800\n","Epoch 76/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.9005 - val_loss: 0.5154 - val_accuracy: 0.8800\n","Epoch 77/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4043 - accuracy: 0.9020 - val_loss: 0.5136 - val_accuracy: 0.8800\n","Epoch 78/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.9005 - val_loss: 0.5117 - val_accuracy: 0.8800\n","Epoch 79/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3993 - accuracy: 0.9025 - val_loss: 0.5095 - val_accuracy: 0.8800\n","Epoch 80/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3969 - accuracy: 0.9025 - val_loss: 0.5076 - val_accuracy: 0.8800\n","Epoch 81/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3945 - accuracy: 0.9015 - val_loss: 0.5059 - val_accuracy: 0.8800\n","Epoch 82/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3922 - accuracy: 0.9025 - val_loss: 0.5044 - val_accuracy: 0.8800\n","Epoch 83/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3899 - accuracy: 0.9030 - val_loss: 0.5034 - val_accuracy: 0.8800\n","Epoch 84/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.9025 - val_loss: 0.5021 - val_accuracy: 0.8800\n","Epoch 85/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3854 - accuracy: 0.9010 - val_loss: 0.5005 - val_accuracy: 0.8800\n","Epoch 86/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3834 - accuracy: 0.9030 - val_loss: 0.4986 - val_accuracy: 0.8800\n","Epoch 87/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3812 - accuracy: 0.9015 - val_loss: 0.4973 - val_accuracy: 0.8800\n","Epoch 88/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3792 - accuracy: 0.9035 - val_loss: 0.4956 - val_accuracy: 0.8800\n","Epoch 89/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3771 - accuracy: 0.9050 - val_loss: 0.4944 - val_accuracy: 0.8800\n","Epoch 90/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3751 - accuracy: 0.9030 - val_loss: 0.4929 - val_accuracy: 0.8800\n","Epoch 91/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3732 - accuracy: 0.9045 - val_loss: 0.4920 - val_accuracy: 0.8800\n","Epoch 92/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3713 - accuracy: 0.9050 - val_loss: 0.4914 - val_accuracy: 0.8800\n","Epoch 93/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3694 - accuracy: 0.9055 - val_loss: 0.4900 - val_accuracy: 0.8800\n","Epoch 94/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3675 - accuracy: 0.9065 - val_loss: 0.4886 - val_accuracy: 0.8800\n","Epoch 95/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.9070 - val_loss: 0.4876 - val_accuracy: 0.8800\n","Epoch 96/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3639 - accuracy: 0.9070 - val_loss: 0.4861 - val_accuracy: 0.8800\n","Epoch 97/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.9080 - val_loss: 0.4851 - val_accuracy: 0.8800\n","Epoch 98/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3603 - accuracy: 0.9070 - val_loss: 0.4840 - val_accuracy: 0.8800\n","Epoch 99/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3586 - accuracy: 0.9080 - val_loss: 0.4834 - val_accuracy: 0.8850\n","Epoch 100/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3569 - accuracy: 0.9075 - val_loss: 0.4821 - val_accuracy: 0.8850\n","Largemodel Test loss: 0.4431287944316864\n","Largemodel Test accuracy: 0.8736000061035156\n"]}],"source":["#initialize weights\n","model.set_weights(weights)\n","\n","large_model_history=model.fit(large_x_train, large_y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_validation, y_validation))\n","\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Largemodel Test loss:', score[0])\n","print('Largemodel Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{"id":"6j_-85YFDA6y"},"source":["##4.2 Dropout\n","Dropout은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 Dropout layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 Dropout model을 만들어 보세요.\n","\n","| Layer (type) | Output Shape | Param # |\n","|------|------|------|\n","| Flatten | (None, 784) | 0 |\n","| Dense | (None, 1024) | 803840 |\n","| Dense | (None, 1024) | 1049600 |\n","| Dense | (None, 1024) | 1049600 |\n","| Dropout | (None, 1024) | 0 |\n","| Flatten | (None, 1024) | 0 |\n","| Dense | (None, 10) | 10250 |"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvYHpRcVDX22","outputId":"95992a09-1174-4b66-9487-b785f5216752","executionInfo":{"status":"ok","timestamp":1679993285811,"user_tz":-540,"elapsed":19,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_2 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 1024)              803840    \n","                                                                 \n"," dense_5 (Dense)             (None, 1024)              1049600   \n","                                                                 \n"," dense_6 (Dense)             (None, 1024)              1049600   \n","                                                                 \n"," dropout (Dropout)           (None, 1024)              0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 1024)              0         \n","                                                                 \n"," dense_7 (Dense)             (None, 10)                10250     \n","                                                                 \n","=================================================================\n","Total params: 2,913,290\n","Trainable params: 2,913,290\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["dropout_model = Sequential()\n","### START CODE HERE ###\n","dropout_model.add(Flatten(input_shape=(28,28)))\n","dropout_model.add(Dense(1024))\n","dropout_model.add(Dense(1024))\n","dropout_model.add(Dense(1024))\n","dropout_model.add(Dropout(0.2))\n","dropout_model.add(Flatten())\n","dropout_model.add(Dense(10, activation='softmax'))\n","### END CODE HERE ###\n","\n","dropout_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"iSuxjOGWEKo9"},"source":["Dropout Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?\n","\n","기존모델과 dropout을 적용한 모델에 대해서, train 데이터와 test데이터에 대한 accuracy차이를 주목해보시면 좋을 것같습니다!"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fqMK-jsyED_h","outputId":"218c70eb-a5f5-4718-a6f9-5a552bbc1223","executionInfo":{"status":"ok","timestamp":1679993324957,"user_tz":-540,"elapsed":39151,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","72/72 [==============================] - 1s 7ms/step - loss: 2.2954 - accuracy: 0.1515 - val_loss: 2.1920 - val_accuracy: 0.2350\n","Epoch 2/100\n","72/72 [==============================] - 0s 6ms/step - loss: 2.1613 - accuracy: 0.2510 - val_loss: 2.0665 - val_accuracy: 0.3800\n","Epoch 3/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.0368 - accuracy: 0.3460 - val_loss: 1.9519 - val_accuracy: 0.4850\n","Epoch 4/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.9192 - accuracy: 0.4510 - val_loss: 1.8480 - val_accuracy: 0.5550\n","Epoch 5/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.8274 - accuracy: 0.5105 - val_loss: 1.7536 - val_accuracy: 0.6050\n","Epoch 6/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.7313 - accuracy: 0.5610 - val_loss: 1.6671 - val_accuracy: 0.6250\n","Epoch 7/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.6415 - accuracy: 0.6025 - val_loss: 1.5879 - val_accuracy: 0.6500\n","Epoch 8/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.5562 - accuracy: 0.6310 - val_loss: 1.5133 - val_accuracy: 0.6800\n","Epoch 9/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.4848 - accuracy: 0.6625 - val_loss: 1.4452 - val_accuracy: 0.6950\n","Epoch 10/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.4135 - accuracy: 0.6905 - val_loss: 1.3843 - val_accuracy: 0.7100\n","Epoch 11/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.3537 - accuracy: 0.6935 - val_loss: 1.3267 - val_accuracy: 0.7250\n","Epoch 12/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.2963 - accuracy: 0.7095 - val_loss: 1.2746 - val_accuracy: 0.7350\n","Epoch 13/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.2359 - accuracy: 0.7305 - val_loss: 1.2258 - val_accuracy: 0.7500\n","Epoch 14/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.1944 - accuracy: 0.7345 - val_loss: 1.1803 - val_accuracy: 0.7650\n","Epoch 15/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.1408 - accuracy: 0.7435 - val_loss: 1.1397 - val_accuracy: 0.7650\n","Epoch 16/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.1037 - accuracy: 0.7575 - val_loss: 1.1020 - val_accuracy: 0.7700\n","Epoch 17/100\n","72/72 [==============================] - 0s 4ms/step - loss: 1.0648 - accuracy: 0.7655 - val_loss: 1.0670 - val_accuracy: 0.7700\n","Epoch 18/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.0272 - accuracy: 0.7705 - val_loss: 1.0346 - val_accuracy: 0.7800\n","Epoch 19/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.9923 - accuracy: 0.7745 - val_loss: 1.0048 - val_accuracy: 0.7800\n","Epoch 20/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.9552 - accuracy: 0.7880 - val_loss: 0.9775 - val_accuracy: 0.7850\n","Epoch 21/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.9325 - accuracy: 0.7855 - val_loss: 0.9512 - val_accuracy: 0.7950\n","Epoch 22/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.9013 - accuracy: 0.8000 - val_loss: 0.9277 - val_accuracy: 0.7950\n","Epoch 23/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.8706 - accuracy: 0.8050 - val_loss: 0.9056 - val_accuracy: 0.7950\n","Epoch 24/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.8579 - accuracy: 0.8080 - val_loss: 0.8851 - val_accuracy: 0.7950\n","Epoch 25/100\n","72/72 [==============================] - 0s 7ms/step - loss: 0.8371 - accuracy: 0.8125 - val_loss: 0.8665 - val_accuracy: 0.7950\n","Epoch 26/100\n","72/72 [==============================] - 1s 9ms/step - loss: 0.8151 - accuracy: 0.8135 - val_loss: 0.8491 - val_accuracy: 0.7950\n","Epoch 27/100\n","72/72 [==============================] - 1s 9ms/step - loss: 0.7948 - accuracy: 0.8225 - val_loss: 0.8325 - val_accuracy: 0.8050\n","Epoch 28/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.7833 - accuracy: 0.8190 - val_loss: 0.8170 - val_accuracy: 0.8100\n","Epoch 29/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.7588 - accuracy: 0.8275 - val_loss: 0.8021 - val_accuracy: 0.8150\n","Epoch 30/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.7472 - accuracy: 0.8310 - val_loss: 0.7891 - val_accuracy: 0.8150\n","Epoch 31/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.7289 - accuracy: 0.8265 - val_loss: 0.7757 - val_accuracy: 0.8200\n","Epoch 32/100\n","72/72 [==============================] - 1s 11ms/step - loss: 0.7132 - accuracy: 0.8425 - val_loss: 0.7642 - val_accuracy: 0.8300\n","Epoch 33/100\n","72/72 [==============================] - 1s 9ms/step - loss: 0.6981 - accuracy: 0.8345 - val_loss: 0.7524 - val_accuracy: 0.8300\n","Epoch 34/100\n","72/72 [==============================] - 1s 10ms/step - loss: 0.6816 - accuracy: 0.8480 - val_loss: 0.7417 - val_accuracy: 0.8350\n","Epoch 35/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.6792 - accuracy: 0.8350 - val_loss: 0.7312 - val_accuracy: 0.8300\n","Epoch 36/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6602 - accuracy: 0.8435 - val_loss: 0.7212 - val_accuracy: 0.8350\n","Epoch 37/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6542 - accuracy: 0.8450 - val_loss: 0.7114 - val_accuracy: 0.8350\n","Epoch 38/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6417 - accuracy: 0.8485 - val_loss: 0.7020 - val_accuracy: 0.8400\n","Epoch 39/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6302 - accuracy: 0.8495 - val_loss: 0.6939 - val_accuracy: 0.8400\n","Epoch 40/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6239 - accuracy: 0.8510 - val_loss: 0.6864 - val_accuracy: 0.8400\n","Epoch 41/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6190 - accuracy: 0.8510 - val_loss: 0.6782 - val_accuracy: 0.8400\n","Epoch 42/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.6088 - accuracy: 0.8475 - val_loss: 0.6715 - val_accuracy: 0.8400\n","Epoch 43/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.8580 - val_loss: 0.6645 - val_accuracy: 0.8400\n","Epoch 44/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5881 - accuracy: 0.8590 - val_loss: 0.6579 - val_accuracy: 0.8400\n","Epoch 45/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.8555 - val_loss: 0.6513 - val_accuracy: 0.8400\n","Epoch 46/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5767 - accuracy: 0.8635 - val_loss: 0.6454 - val_accuracy: 0.8400\n","Epoch 47/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5710 - accuracy: 0.8650 - val_loss: 0.6397 - val_accuracy: 0.8400\n","Epoch 48/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5605 - accuracy: 0.8635 - val_loss: 0.6343 - val_accuracy: 0.8400\n","Epoch 49/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5569 - accuracy: 0.8660 - val_loss: 0.6293 - val_accuracy: 0.8400\n","Epoch 50/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5487 - accuracy: 0.8705 - val_loss: 0.6245 - val_accuracy: 0.8400\n","Epoch 51/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5455 - accuracy: 0.8690 - val_loss: 0.6193 - val_accuracy: 0.8450\n","Epoch 52/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5365 - accuracy: 0.8695 - val_loss: 0.6149 - val_accuracy: 0.8450\n","Epoch 53/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5282 - accuracy: 0.8700 - val_loss: 0.6103 - val_accuracy: 0.8450\n","Epoch 54/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5229 - accuracy: 0.8710 - val_loss: 0.6059 - val_accuracy: 0.8450\n","Epoch 55/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5192 - accuracy: 0.8755 - val_loss: 0.6022 - val_accuracy: 0.8450\n","Epoch 56/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5147 - accuracy: 0.8740 - val_loss: 0.5975 - val_accuracy: 0.8450\n","Epoch 57/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5089 - accuracy: 0.8745 - val_loss: 0.5934 - val_accuracy: 0.8450\n","Epoch 58/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.5068 - accuracy: 0.8755 - val_loss: 0.5899 - val_accuracy: 0.8450\n","Epoch 59/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4999 - accuracy: 0.8780 - val_loss: 0.5866 - val_accuracy: 0.8450\n","Epoch 60/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4965 - accuracy: 0.8795 - val_loss: 0.5829 - val_accuracy: 0.8450\n","Epoch 61/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4910 - accuracy: 0.8780 - val_loss: 0.5791 - val_accuracy: 0.8450\n","Epoch 62/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4861 - accuracy: 0.8745 - val_loss: 0.5759 - val_accuracy: 0.8450\n","Epoch 63/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4860 - accuracy: 0.8810 - val_loss: 0.5729 - val_accuracy: 0.8450\n","Epoch 64/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.8765 - val_loss: 0.5696 - val_accuracy: 0.8500\n","Epoch 65/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4775 - accuracy: 0.8850 - val_loss: 0.5662 - val_accuracy: 0.8500\n","Epoch 66/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4718 - accuracy: 0.8835 - val_loss: 0.5632 - val_accuracy: 0.8500\n","Epoch 67/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4667 - accuracy: 0.8865 - val_loss: 0.5608 - val_accuracy: 0.8500\n","Epoch 68/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4607 - accuracy: 0.8890 - val_loss: 0.5582 - val_accuracy: 0.8500\n","Epoch 69/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4609 - accuracy: 0.8865 - val_loss: 0.5557 - val_accuracy: 0.8500\n","Epoch 70/100\n","72/72 [==============================] - 1s 11ms/step - loss: 0.4595 - accuracy: 0.8800 - val_loss: 0.5535 - val_accuracy: 0.8500\n","Epoch 71/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.4567 - accuracy: 0.8870 - val_loss: 0.5512 - val_accuracy: 0.8500\n","Epoch 72/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.8850 - val_loss: 0.5486 - val_accuracy: 0.8500\n","Epoch 73/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4451 - accuracy: 0.8935 - val_loss: 0.5461 - val_accuracy: 0.8500\n","Epoch 74/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4501 - accuracy: 0.8830 - val_loss: 0.5439 - val_accuracy: 0.8500\n","Epoch 75/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4411 - accuracy: 0.8855 - val_loss: 0.5416 - val_accuracy: 0.8500\n","Epoch 76/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4363 - accuracy: 0.8880 - val_loss: 0.5397 - val_accuracy: 0.8500\n","Epoch 77/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4354 - accuracy: 0.8910 - val_loss: 0.5371 - val_accuracy: 0.8500\n","Epoch 78/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4361 - accuracy: 0.8890 - val_loss: 0.5356 - val_accuracy: 0.8500\n","Epoch 79/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4267 - accuracy: 0.8855 - val_loss: 0.5335 - val_accuracy: 0.8500\n","Epoch 80/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.8900 - val_loss: 0.5312 - val_accuracy: 0.8500\n","Epoch 81/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.8930 - val_loss: 0.5293 - val_accuracy: 0.8500\n","Epoch 82/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4231 - accuracy: 0.8935 - val_loss: 0.5271 - val_accuracy: 0.8500\n","Epoch 83/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4177 - accuracy: 0.8930 - val_loss: 0.5258 - val_accuracy: 0.8500\n","Epoch 84/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4241 - accuracy: 0.8880 - val_loss: 0.5239 - val_accuracy: 0.8500\n","Epoch 85/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4145 - accuracy: 0.8935 - val_loss: 0.5220 - val_accuracy: 0.8500\n","Epoch 86/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4143 - accuracy: 0.8950 - val_loss: 0.5198 - val_accuracy: 0.8500\n","Epoch 87/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4101 - accuracy: 0.8935 - val_loss: 0.5184 - val_accuracy: 0.8500\n","Epoch 88/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4099 - accuracy: 0.8955 - val_loss: 0.5171 - val_accuracy: 0.8500\n","Epoch 89/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4065 - accuracy: 0.8945 - val_loss: 0.5156 - val_accuracy: 0.8500\n","Epoch 90/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4020 - accuracy: 0.8925 - val_loss: 0.5144 - val_accuracy: 0.8500\n","Epoch 91/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8975 - val_loss: 0.5126 - val_accuracy: 0.8500\n","Epoch 92/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.8915 - val_loss: 0.5111 - val_accuracy: 0.8500\n","Epoch 93/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3958 - accuracy: 0.8965 - val_loss: 0.5099 - val_accuracy: 0.8500\n","Epoch 94/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3953 - accuracy: 0.8985 - val_loss: 0.5086 - val_accuracy: 0.8500\n","Epoch 95/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8980 - val_loss: 0.5066 - val_accuracy: 0.8500\n","Epoch 96/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8975 - val_loss: 0.5055 - val_accuracy: 0.8500\n","Epoch 97/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3916 - accuracy: 0.9015 - val_loss: 0.5044 - val_accuracy: 0.8500\n","Epoch 98/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.9025 - val_loss: 0.5029 - val_accuracy: 0.8500\n","Epoch 99/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3868 - accuracy: 0.9055 - val_loss: 0.5016 - val_accuracy: 0.8500\n","Epoch 100/100\n","72/72 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.8985 - val_loss: 0.5006 - val_accuracy: 0.8500\n","Dropout model Test loss: 0.4519326686859131\n","Dropout model Test accuracy: 0.8751999735832214\n"]}],"source":["dropout_model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","\n","dropout_model_history=dropout_model.fit(large_x_train, large_y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_validation, y_validation))\n","\n","score = dropout_model.evaluate(x_test, y_test, verbose=0)\n","print('Dropout model Test loss:', score[0])\n","print('Dropout model Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{"id":"AEyhMagnGq8F"},"source":["##4.3 BatchNormalization\n","BatchNormalization(BN)은 쉽게 쓸 수 있는 Regularization 기법입니다. Layer 사이에 BN layer만 추가하면 되기 때문에 간편합니다. 다음과 같은 BN model을 만들어 보세요.\n","\n","| Layer (type) | Output Shape | Param # |\n","|------|------|------|\n","| Flatten | (None, 784) | 0 |\n","| Dense | (None, 1024) | 803840 |\n","| BatchNormalization | (None, 1024) | 4096 |\n","| Activation | (None, 1024) | 0 |\n","| Dense | (None, 1024) | 1049600 |\n","| BatchNormalization | (None, 1024) | 4096 |\n","| Activation | (None, 1024) | 0 |\n","| Dense | (None, 1024) | 1049600 |\n","| BatchNormalization | (None, 1024) | 4096 |\n","| Activation | (None, 1024) | 0 |\n","| Flatten | (None, 1024) | 0 |\n","| Dense | (None, 10) | 10250 |"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sOIBMssiBJaB","outputId":"dab78c08-532f-4ff7-9e27-4dd343737b24","executionInfo":{"status":"ok","timestamp":1679993533971,"user_tz":-540,"elapsed":1241,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_9 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_17 (Dense)            (None, 1024)              803840    \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_6 (Activation)   (None, 1024)              0         \n","                                                                 \n"," dense_18 (Dense)            (None, 1024)              1049600   \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_7 (Activation)   (None, 1024)              0         \n","                                                                 \n"," dense_19 (Dense)            (None, 1024)              1049600   \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_8 (Activation)   (None, 1024)              0         \n","                                                                 \n"," flatten_10 (Flatten)        (None, 1024)              0         \n","                                                                 \n"," dense_20 (Dense)            (None, 10)                10250     \n","                                                                 \n","=================================================================\n","Total params: 2,925,578\n","Trainable params: 2,919,434\n","Non-trainable params: 6,144\n","_________________________________________________________________\n"]}],"source":["bn_model = Sequential()\n","### START CODE HERE ###\n","bn_model.add(Flatten(input_shape=(28,28)))\n","\n","bn_model.add(Dense(1024))\n","bn_model.add(BatchNormalization())\n","bn_model.add(Activation(activation='relu'))\n","bn_model.add(Dense(1024))\n","bn_model.add(BatchNormalization())\n","bn_model.add(Activation(activation='relu'))\n","bn_model.add(Dense(1024))\n","bn_model.add(BatchNormalization())\n","bn_model.add(Activation(activation='relu'))\n","\n","bn_model.add(Flatten())\n","bn_model.add(Dense(10, activation='softmax'))\n","### END CODE HERE ###\n","\n","bn_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"izQBrVW6KYr3"},"source":["BN Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bL2oakRaKZ4y","outputId":"1ec2d500-0440-4f17-a6db-4b2ce6dce457","executionInfo":{"status":"ok","timestamp":1679993622450,"user_tz":-540,"elapsed":84178,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","72/72 [==============================] - 1s 8ms/step - loss: 2.6653 - accuracy: 0.1020 - val_loss: 2.2770 - val_accuracy: 0.1250\n","Epoch 2/100\n","72/72 [==============================] - 0s 6ms/step - loss: 2.4562 - accuracy: 0.1420 - val_loss: 2.2465 - val_accuracy: 0.1350\n","Epoch 3/100\n","72/72 [==============================] - 0s 6ms/step - loss: 2.2799 - accuracy: 0.1930 - val_loss: 2.1581 - val_accuracy: 0.1800\n","Epoch 4/100\n","72/72 [==============================] - 0s 6ms/step - loss: 2.1131 - accuracy: 0.2430 - val_loss: 2.0264 - val_accuracy: 0.2600\n","Epoch 5/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.9639 - accuracy: 0.3115 - val_loss: 1.8914 - val_accuracy: 0.3350\n","Epoch 6/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.8301 - accuracy: 0.3670 - val_loss: 1.7709 - val_accuracy: 0.4150\n","Epoch 7/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.7271 - accuracy: 0.4210 - val_loss: 1.6625 - val_accuracy: 0.4350\n","Epoch 8/100\n","72/72 [==============================] - 1s 7ms/step - loss: 1.6289 - accuracy: 0.4590 - val_loss: 1.5637 - val_accuracy: 0.4900\n","Epoch 9/100\n","72/72 [==============================] - 0s 7ms/step - loss: 1.5396 - accuracy: 0.5055 - val_loss: 1.4728 - val_accuracy: 0.5450\n","Epoch 10/100\n","72/72 [==============================] - 0s 7ms/step - loss: 1.4585 - accuracy: 0.5470 - val_loss: 1.3908 - val_accuracy: 0.5950\n","Epoch 11/100\n","72/72 [==============================] - 1s 7ms/step - loss: 1.3673 - accuracy: 0.5870 - val_loss: 1.3171 - val_accuracy: 0.6050\n","Epoch 12/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.2891 - accuracy: 0.6270 - val_loss: 1.2495 - val_accuracy: 0.6200\n","Epoch 13/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.2307 - accuracy: 0.6420 - val_loss: 1.1885 - val_accuracy: 0.6450\n","Epoch 14/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.1734 - accuracy: 0.6670 - val_loss: 1.1347 - val_accuracy: 0.6650\n","Epoch 15/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.1081 - accuracy: 0.6985 - val_loss: 1.0855 - val_accuracy: 0.6850\n","Epoch 16/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.0737 - accuracy: 0.6965 - val_loss: 1.0393 - val_accuracy: 0.7100\n","Epoch 17/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.0188 - accuracy: 0.7200 - val_loss: 0.9989 - val_accuracy: 0.7100\n","Epoch 18/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.9765 - accuracy: 0.7475 - val_loss: 0.9628 - val_accuracy: 0.7250\n","Epoch 19/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.9509 - accuracy: 0.7475 - val_loss: 0.9263 - val_accuracy: 0.7500\n","Epoch 20/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.9149 - accuracy: 0.7600 - val_loss: 0.8952 - val_accuracy: 0.7500\n","Epoch 21/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.8729 - accuracy: 0.7775 - val_loss: 0.8662 - val_accuracy: 0.7500\n","Epoch 22/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.8253 - accuracy: 0.7905 - val_loss: 0.8408 - val_accuracy: 0.7500\n","Epoch 23/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.8148 - accuracy: 0.7900 - val_loss: 0.8158 - val_accuracy: 0.7550\n","Epoch 24/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7848 - accuracy: 0.8010 - val_loss: 0.7936 - val_accuracy: 0.7600\n","Epoch 25/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7551 - accuracy: 0.8150 - val_loss: 0.7717 - val_accuracy: 0.7750\n","Epoch 26/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7361 - accuracy: 0.8145 - val_loss: 0.7513 - val_accuracy: 0.7750\n","Epoch 27/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.7138 - accuracy: 0.8170 - val_loss: 0.7327 - val_accuracy: 0.7800\n","Epoch 28/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.8360 - val_loss: 0.7151 - val_accuracy: 0.7850\n","Epoch 29/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.6840 - accuracy: 0.8335 - val_loss: 0.6981 - val_accuracy: 0.7900\n","Epoch 30/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6603 - accuracy: 0.8430 - val_loss: 0.6829 - val_accuracy: 0.7950\n","Epoch 31/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6369 - accuracy: 0.8470 - val_loss: 0.6698 - val_accuracy: 0.8050\n","Epoch 32/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6099 - accuracy: 0.8580 - val_loss: 0.6557 - val_accuracy: 0.8100\n","Epoch 33/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6146 - accuracy: 0.8660 - val_loss: 0.6425 - val_accuracy: 0.8150\n","Epoch 34/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.5914 - accuracy: 0.8585 - val_loss: 0.6303 - val_accuracy: 0.8200\n","Epoch 35/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.5718 - accuracy: 0.8650 - val_loss: 0.6194 - val_accuracy: 0.8200\n","Epoch 36/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5654 - accuracy: 0.8590 - val_loss: 0.6096 - val_accuracy: 0.8200\n","Epoch 37/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.5581 - accuracy: 0.8725 - val_loss: 0.5990 - val_accuracy: 0.8200\n","Epoch 38/100\n","72/72 [==============================] - 0s 7ms/step - loss: 0.5423 - accuracy: 0.8795 - val_loss: 0.5884 - val_accuracy: 0.8250\n","Epoch 39/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.5333 - accuracy: 0.8780 - val_loss: 0.5793 - val_accuracy: 0.8350\n","Epoch 40/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.5277 - accuracy: 0.8690 - val_loss: 0.5710 - val_accuracy: 0.8400\n","Epoch 41/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.5177 - accuracy: 0.8780 - val_loss: 0.5633 - val_accuracy: 0.8400\n","Epoch 42/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4961 - accuracy: 0.8910 - val_loss: 0.5552 - val_accuracy: 0.8400\n","Epoch 43/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4854 - accuracy: 0.8870 - val_loss: 0.5477 - val_accuracy: 0.8400\n","Epoch 44/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4806 - accuracy: 0.8850 - val_loss: 0.5404 - val_accuracy: 0.8400\n","Epoch 45/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4670 - accuracy: 0.8935 - val_loss: 0.5332 - val_accuracy: 0.8450\n","Epoch 46/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4640 - accuracy: 0.8995 - val_loss: 0.5274 - val_accuracy: 0.8450\n","Epoch 47/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4559 - accuracy: 0.8950 - val_loss: 0.5211 - val_accuracy: 0.8450\n","Epoch 48/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4541 - accuracy: 0.8935 - val_loss: 0.5144 - val_accuracy: 0.8450\n","Epoch 49/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4360 - accuracy: 0.9015 - val_loss: 0.5088 - val_accuracy: 0.8450\n","Epoch 50/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4243 - accuracy: 0.9100 - val_loss: 0.5030 - val_accuracy: 0.8550\n","Epoch 51/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.9045 - val_loss: 0.4983 - val_accuracy: 0.8600\n","Epoch 52/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.9115 - val_loss: 0.4931 - val_accuracy: 0.8550\n","Epoch 53/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3993 - accuracy: 0.9120 - val_loss: 0.4875 - val_accuracy: 0.8600\n","Epoch 54/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3994 - accuracy: 0.9130 - val_loss: 0.4825 - val_accuracy: 0.8600\n","Epoch 55/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3997 - accuracy: 0.9160 - val_loss: 0.4782 - val_accuracy: 0.8600\n","Epoch 56/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3904 - accuracy: 0.9220 - val_loss: 0.4734 - val_accuracy: 0.8600\n","Epoch 57/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3768 - accuracy: 0.9125 - val_loss: 0.4694 - val_accuracy: 0.8600\n","Epoch 58/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3747 - accuracy: 0.9265 - val_loss: 0.4645 - val_accuracy: 0.8600\n","Epoch 59/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3738 - accuracy: 0.9235 - val_loss: 0.4616 - val_accuracy: 0.8600\n","Epoch 60/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3663 - accuracy: 0.9240 - val_loss: 0.4571 - val_accuracy: 0.8600\n","Epoch 61/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.9240 - val_loss: 0.4529 - val_accuracy: 0.8650\n","Epoch 62/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3608 - accuracy: 0.9295 - val_loss: 0.4495 - val_accuracy: 0.8650\n","Epoch 63/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3560 - accuracy: 0.9270 - val_loss: 0.4463 - val_accuracy: 0.8650\n","Epoch 64/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3381 - accuracy: 0.9225 - val_loss: 0.4432 - val_accuracy: 0.8650\n","Epoch 65/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3334 - accuracy: 0.9345 - val_loss: 0.4402 - val_accuracy: 0.8650\n","Epoch 66/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.3276 - accuracy: 0.9365 - val_loss: 0.4362 - val_accuracy: 0.8700\n","Epoch 67/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.3336 - accuracy: 0.9310 - val_loss: 0.4332 - val_accuracy: 0.8750\n","Epoch 68/100\n","72/72 [==============================] - 0s 7ms/step - loss: 0.3225 - accuracy: 0.9420 - val_loss: 0.4302 - val_accuracy: 0.8750\n","Epoch 69/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.3211 - accuracy: 0.9305 - val_loss: 0.4272 - val_accuracy: 0.8800\n","Epoch 70/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3206 - accuracy: 0.9355 - val_loss: 0.4247 - val_accuracy: 0.8800\n","Epoch 71/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2998 - accuracy: 0.9365 - val_loss: 0.4213 - val_accuracy: 0.8800\n","Epoch 72/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3049 - accuracy: 0.9370 - val_loss: 0.4185 - val_accuracy: 0.8800\n","Epoch 73/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3137 - accuracy: 0.9390 - val_loss: 0.4165 - val_accuracy: 0.8800\n","Epoch 74/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2925 - accuracy: 0.9415 - val_loss: 0.4138 - val_accuracy: 0.8800\n","Epoch 75/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2960 - accuracy: 0.9445 - val_loss: 0.4114 - val_accuracy: 0.8850\n","Epoch 76/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2849 - accuracy: 0.9455 - val_loss: 0.4086 - val_accuracy: 0.8900\n","Epoch 77/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2852 - accuracy: 0.9475 - val_loss: 0.4064 - val_accuracy: 0.8950\n","Epoch 78/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2765 - accuracy: 0.9475 - val_loss: 0.4035 - val_accuracy: 0.8900\n","Epoch 79/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2819 - accuracy: 0.9425 - val_loss: 0.4006 - val_accuracy: 0.8900\n","Epoch 80/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2722 - accuracy: 0.9480 - val_loss: 0.3989 - val_accuracy: 0.8900\n","Epoch 81/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2815 - accuracy: 0.9430 - val_loss: 0.3959 - val_accuracy: 0.8900\n","Epoch 82/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2604 - accuracy: 0.9505 - val_loss: 0.3948 - val_accuracy: 0.8900\n","Epoch 83/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2596 - accuracy: 0.9480 - val_loss: 0.3927 - val_accuracy: 0.8950\n","Epoch 84/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2691 - accuracy: 0.9490 - val_loss: 0.3903 - val_accuracy: 0.8900\n","Epoch 85/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2570 - accuracy: 0.9585 - val_loss: 0.3876 - val_accuracy: 0.8900\n","Epoch 86/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2551 - accuracy: 0.9540 - val_loss: 0.3857 - val_accuracy: 0.8900\n","Epoch 87/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2530 - accuracy: 0.9510 - val_loss: 0.3847 - val_accuracy: 0.8950\n","Epoch 88/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2536 - accuracy: 0.9530 - val_loss: 0.3830 - val_accuracy: 0.8950\n","Epoch 89/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2352 - accuracy: 0.9590 - val_loss: 0.3800 - val_accuracy: 0.8950\n","Epoch 90/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2407 - accuracy: 0.9525 - val_loss: 0.3785 - val_accuracy: 0.9000\n","Epoch 91/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2381 - accuracy: 0.9550 - val_loss: 0.3761 - val_accuracy: 0.9000\n","Epoch 92/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2438 - accuracy: 0.9525 - val_loss: 0.3743 - val_accuracy: 0.8950\n","Epoch 93/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2358 - accuracy: 0.9535 - val_loss: 0.3729 - val_accuracy: 0.9000\n","Epoch 94/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.2269 - accuracy: 0.9605 - val_loss: 0.3718 - val_accuracy: 0.9000\n","Epoch 95/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.2244 - accuracy: 0.9590 - val_loss: 0.3708 - val_accuracy: 0.9000\n","Epoch 96/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.2190 - accuracy: 0.9575 - val_loss: 0.3680 - val_accuracy: 0.9000\n","Epoch 97/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.2216 - accuracy: 0.9580 - val_loss: 0.3662 - val_accuracy: 0.9000\n","Epoch 98/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2227 - accuracy: 0.9605 - val_loss: 0.3647 - val_accuracy: 0.9050\n","Epoch 99/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2150 - accuracy: 0.9575 - val_loss: 0.3633 - val_accuracy: 0.9050\n","Epoch 100/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2169 - accuracy: 0.9645 - val_loss: 0.3608 - val_accuracy: 0.9100\n","BN model Test loss: 0.3746321499347687\n","BN model Test accuracy: 0.892799973487854\n"]}],"source":["bn_model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","bn_model_history=bn_model.fit(large_x_train, large_y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_validation, y_validation))\n","\n","score = bn_model.evaluate(x_test, y_test, verbose=0)\n","print('BN model Test loss:', score[0])\n","print('BN model Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{"id":"5fDPwaFsJ47I"},"source":["##4.4 Final Model\n","지금까지 썼던 Regularization 기법들을 종합선물세트로 적용해 봅시다. 다음과 같은 Final model을 만들어 보세요.\n","\n","| Layer (type) | Output Shape | Param # |\n","|------|------|------|\n","| Flatten | (None, 784) | 0 |\n","| Dense | (None, 1024) | 803840 |\n","| BatchNormalization | (None, 1024) | 4096 |\n","| Activation | (None, 1024) | 0 |\n","| Dense | (None, 1024) | 1049600 |\n","| BatchNormalization | (None, 1024) | 4096 |\n","| Activation | (None, 1024) | 0 |\n","| Dense | (None, 1024) | 1049600 |\n","| BatchNormalization | (None, 1024) | 4096 |\n","| Activation | (None, 1024) | 0 |\n","| Dropout | (None, 1024) | 0 |\n","| Flatten | (None, 1024) | 0 |\n","| Dense | (None, 10) | 10250 |"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8ppnPcELADo","outputId":"ee68fbed-c1fa-469c-eca9-4b8674c9c0d3","executionInfo":{"status":"ok","timestamp":1679993522606,"user_tz":-540,"elapsed":14,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_7 (Flatten)         (None, 784)               0         \n","                                                                 \n"," dense_13 (Dense)            (None, 1024)              803840    \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_3 (Activation)   (None, 1024)              0         \n","                                                                 \n"," dense_14 (Dense)            (None, 1024)              1049600   \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_4 (Activation)   (None, 1024)              0         \n","                                                                 \n"," dense_15 (Dense)            (None, 1024)              1049600   \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," activation_5 (Activation)   (None, 1024)              0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 1024)              0         \n","                                                                 \n"," flatten_8 (Flatten)         (None, 1024)              0         \n","                                                                 \n"," dense_16 (Dense)            (None, 10)                10250     \n","                                                                 \n","=================================================================\n","Total params: 2,925,578\n","Trainable params: 2,919,434\n","Non-trainable params: 6,144\n","_________________________________________________________________\n"]}],"source":["final_model = Sequential()\n","### START CODE HERE ###\n","final_model.add(Flatten(input_shape=(28,28)))\n","\n","final_model.add(Dense(1024))\n","final_model.add(BatchNormalization())\n","final_model.add(Activation(activation='relu'))\n","final_model.add(Dense(1024))\n","final_model.add(BatchNormalization())\n","final_model.add(Activation(activation='relu'))\n","final_model.add(Dense(1024))\n","final_model.add(BatchNormalization())\n","final_model.add(Activation(activation='relu'))\n","\n","final_model.add(Dropout(0.2))\n","final_model.add(Flatten())\n","final_model.add(Dense(10, activation='softmax'))\n","### END CODE HERE ###\n","\n","final_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"ocok8VnoLcb4"},"source":["Final Model을 학습해 보겠습니다. Generalization 성능이 올라갔나요?"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A-bFRiHtLdVe","outputId":"dbacf33d-da1a-415e-d157-7f3165cdb29a","executionInfo":{"status":"ok","timestamp":1679993792313,"user_tz":-540,"elapsed":84276,"user":{"displayName":"한준호","userId":"06409990839494728280"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","72/72 [==============================] - 1s 8ms/step - loss: 2.9090 - accuracy: 0.0935 - val_loss: 2.3280 - val_accuracy: 0.1600\n","Epoch 2/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.7418 - accuracy: 0.1090 - val_loss: 2.3181 - val_accuracy: 0.1600\n","Epoch 3/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.5594 - accuracy: 0.1435 - val_loss: 2.2691 - val_accuracy: 0.1800\n","Epoch 4/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.3936 - accuracy: 0.1935 - val_loss: 2.1780 - val_accuracy: 0.2400\n","Epoch 5/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.2743 - accuracy: 0.2175 - val_loss: 2.0635 - val_accuracy: 0.2700\n","Epoch 6/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.1076 - accuracy: 0.2600 - val_loss: 1.9473 - val_accuracy: 0.3300\n","Epoch 7/100\n","72/72 [==============================] - 0s 5ms/step - loss: 2.0067 - accuracy: 0.2865 - val_loss: 1.8365 - val_accuracy: 0.3650\n","Epoch 8/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.8668 - accuracy: 0.3400 - val_loss: 1.7298 - val_accuracy: 0.4350\n","Epoch 9/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.7621 - accuracy: 0.3835 - val_loss: 1.6323 - val_accuracy: 0.4700\n","Epoch 10/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.7034 - accuracy: 0.4055 - val_loss: 1.5407 - val_accuracy: 0.4900\n","Epoch 11/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.5909 - accuracy: 0.4470 - val_loss: 1.4578 - val_accuracy: 0.5150\n","Epoch 12/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.5293 - accuracy: 0.4810 - val_loss: 1.3819 - val_accuracy: 0.5250\n","Epoch 13/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.4233 - accuracy: 0.5250 - val_loss: 1.3152 - val_accuracy: 0.5650\n","Epoch 14/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.3785 - accuracy: 0.5555 - val_loss: 1.2540 - val_accuracy: 0.5950\n","Epoch 15/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.3210 - accuracy: 0.5880 - val_loss: 1.1963 - val_accuracy: 0.6200\n","Epoch 16/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.2501 - accuracy: 0.6085 - val_loss: 1.1447 - val_accuracy: 0.6400\n","Epoch 17/100\n","72/72 [==============================] - 0s 5ms/step - loss: 1.2091 - accuracy: 0.6165 - val_loss: 1.0963 - val_accuracy: 0.6600\n","Epoch 18/100\n","72/72 [==============================] - 0s 6ms/step - loss: 1.1704 - accuracy: 0.6255 - val_loss: 1.0530 - val_accuracy: 0.6850\n","Epoch 19/100\n","72/72 [==============================] - 1s 8ms/step - loss: 1.1157 - accuracy: 0.6505 - val_loss: 1.0110 - val_accuracy: 0.7000\n","Epoch 20/100\n","72/72 [==============================] - 1s 8ms/step - loss: 1.0570 - accuracy: 0.6800 - val_loss: 0.9737 - val_accuracy: 0.7250\n","Epoch 21/100\n","72/72 [==============================] - 1s 8ms/step - loss: 1.0187 - accuracy: 0.6940 - val_loss: 0.9399 - val_accuracy: 0.7350\n","Epoch 22/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.9829 - accuracy: 0.7200 - val_loss: 0.9073 - val_accuracy: 0.7450\n","Epoch 23/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.9576 - accuracy: 0.7335 - val_loss: 0.8769 - val_accuracy: 0.7550\n","Epoch 24/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.9179 - accuracy: 0.7410 - val_loss: 0.8507 - val_accuracy: 0.7700\n","Epoch 25/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.8867 - accuracy: 0.7470 - val_loss: 0.8255 - val_accuracy: 0.7800\n","Epoch 26/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.8714 - accuracy: 0.7490 - val_loss: 0.8021 - val_accuracy: 0.7950\n","Epoch 27/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.8403 - accuracy: 0.7605 - val_loss: 0.7802 - val_accuracy: 0.8050\n","Epoch 28/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.8112 - accuracy: 0.7760 - val_loss: 0.7605 - val_accuracy: 0.8100\n","Epoch 29/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.8148 - accuracy: 0.7710 - val_loss: 0.7412 - val_accuracy: 0.8150\n","Epoch 30/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7739 - accuracy: 0.7870 - val_loss: 0.7221 - val_accuracy: 0.8150\n","Epoch 31/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7628 - accuracy: 0.7990 - val_loss: 0.7047 - val_accuracy: 0.8150\n","Epoch 32/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7398 - accuracy: 0.7970 - val_loss: 0.6883 - val_accuracy: 0.8150\n","Epoch 33/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7094 - accuracy: 0.8160 - val_loss: 0.6726 - val_accuracy: 0.8200\n","Epoch 34/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.7011 - accuracy: 0.8095 - val_loss: 0.6585 - val_accuracy: 0.8250\n","Epoch 35/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6836 - accuracy: 0.8140 - val_loss: 0.6449 - val_accuracy: 0.8250\n","Epoch 36/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6603 - accuracy: 0.8270 - val_loss: 0.6312 - val_accuracy: 0.8350\n","Epoch 37/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.6501 - accuracy: 0.8385 - val_loss: 0.6196 - val_accuracy: 0.8350\n","Epoch 38/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.6418 - accuracy: 0.8285 - val_loss: 0.6091 - val_accuracy: 0.8400\n","Epoch 39/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6177 - accuracy: 0.8405 - val_loss: 0.5987 - val_accuracy: 0.8400\n","Epoch 40/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6217 - accuracy: 0.8405 - val_loss: 0.5883 - val_accuracy: 0.8400\n","Epoch 41/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.6017 - accuracy: 0.8360 - val_loss: 0.5792 - val_accuracy: 0.8450\n","Epoch 42/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5716 - accuracy: 0.8570 - val_loss: 0.5691 - val_accuracy: 0.8450\n","Epoch 43/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5804 - accuracy: 0.8530 - val_loss: 0.5608 - val_accuracy: 0.8450\n","Epoch 44/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.5832 - accuracy: 0.8420 - val_loss: 0.5522 - val_accuracy: 0.8550\n","Epoch 45/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5560 - accuracy: 0.8545 - val_loss: 0.5440 - val_accuracy: 0.8600\n","Epoch 46/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.5336 - accuracy: 0.8625 - val_loss: 0.5365 - val_accuracy: 0.8600\n","Epoch 47/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.5407 - accuracy: 0.8615 - val_loss: 0.5303 - val_accuracy: 0.8700\n","Epoch 48/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.5221 - accuracy: 0.8665 - val_loss: 0.5232 - val_accuracy: 0.8700\n","Epoch 49/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.5176 - accuracy: 0.8650 - val_loss: 0.5163 - val_accuracy: 0.8700\n","Epoch 50/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.5113 - accuracy: 0.8680 - val_loss: 0.5086 - val_accuracy: 0.8750\n","Epoch 51/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4982 - accuracy: 0.8740 - val_loss: 0.5033 - val_accuracy: 0.8750\n","Epoch 52/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4996 - accuracy: 0.8765 - val_loss: 0.4970 - val_accuracy: 0.8850\n","Epoch 53/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4915 - accuracy: 0.8810 - val_loss: 0.4917 - val_accuracy: 0.8900\n","Epoch 54/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4819 - accuracy: 0.8750 - val_loss: 0.4858 - val_accuracy: 0.8900\n","Epoch 55/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4935 - accuracy: 0.8720 - val_loss: 0.4801 - val_accuracy: 0.8900\n","Epoch 56/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4660 - accuracy: 0.8890 - val_loss: 0.4753 - val_accuracy: 0.8900\n","Epoch 57/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4675 - accuracy: 0.8810 - val_loss: 0.4709 - val_accuracy: 0.8900\n","Epoch 58/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4573 - accuracy: 0.8790 - val_loss: 0.4666 - val_accuracy: 0.8900\n","Epoch 59/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4347 - accuracy: 0.8980 - val_loss: 0.4620 - val_accuracy: 0.8900\n","Epoch 60/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4477 - accuracy: 0.8835 - val_loss: 0.4576 - val_accuracy: 0.8900\n","Epoch 61/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4319 - accuracy: 0.8930 - val_loss: 0.4547 - val_accuracy: 0.8900\n","Epoch 62/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4157 - accuracy: 0.8990 - val_loss: 0.4499 - val_accuracy: 0.8900\n","Epoch 63/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4318 - accuracy: 0.8875 - val_loss: 0.4468 - val_accuracy: 0.8950\n","Epoch 64/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4312 - accuracy: 0.8880 - val_loss: 0.4424 - val_accuracy: 0.9000\n","Epoch 65/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.4204 - accuracy: 0.8975 - val_loss: 0.4381 - val_accuracy: 0.8950\n","Epoch 66/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3995 - accuracy: 0.9025 - val_loss: 0.4339 - val_accuracy: 0.8950\n","Epoch 67/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.4051 - accuracy: 0.9010 - val_loss: 0.4293 - val_accuracy: 0.9000\n","Epoch 68/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3944 - accuracy: 0.9030 - val_loss: 0.4260 - val_accuracy: 0.9000\n","Epoch 69/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3771 - accuracy: 0.9040 - val_loss: 0.4234 - val_accuracy: 0.9000\n","Epoch 70/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3774 - accuracy: 0.9065 - val_loss: 0.4206 - val_accuracy: 0.9000\n","Epoch 71/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3941 - accuracy: 0.8990 - val_loss: 0.4175 - val_accuracy: 0.9000\n","Epoch 72/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3755 - accuracy: 0.9050 - val_loss: 0.4149 - val_accuracy: 0.9000\n","Epoch 73/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3813 - accuracy: 0.8965 - val_loss: 0.4114 - val_accuracy: 0.9000\n","Epoch 74/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3536 - accuracy: 0.9085 - val_loss: 0.4088 - val_accuracy: 0.9000\n","Epoch 75/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3565 - accuracy: 0.9145 - val_loss: 0.4060 - val_accuracy: 0.9000\n","Epoch 76/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.3678 - accuracy: 0.9075 - val_loss: 0.4033 - val_accuracy: 0.9000\n","Epoch 77/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.3580 - accuracy: 0.9095 - val_loss: 0.4018 - val_accuracy: 0.9000\n","Epoch 78/100\n","72/72 [==============================] - 1s 7ms/step - loss: 0.3475 - accuracy: 0.9175 - val_loss: 0.3984 - val_accuracy: 0.9050\n","Epoch 79/100\n","72/72 [==============================] - 1s 8ms/step - loss: 0.3455 - accuracy: 0.9070 - val_loss: 0.3957 - val_accuracy: 0.9050\n","Epoch 80/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3426 - accuracy: 0.9125 - val_loss: 0.3944 - val_accuracy: 0.9050\n","Epoch 81/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3348 - accuracy: 0.9185 - val_loss: 0.3916 - val_accuracy: 0.9050\n","Epoch 82/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3417 - accuracy: 0.9180 - val_loss: 0.3882 - val_accuracy: 0.9050\n","Epoch 83/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3227 - accuracy: 0.9205 - val_loss: 0.3869 - val_accuracy: 0.9050\n","Epoch 84/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3349 - accuracy: 0.9205 - val_loss: 0.3853 - val_accuracy: 0.9050\n","Epoch 85/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.9175 - val_loss: 0.3833 - val_accuracy: 0.9050\n","Epoch 86/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3173 - accuracy: 0.9220 - val_loss: 0.3812 - val_accuracy: 0.9050\n","Epoch 87/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3131 - accuracy: 0.9215 - val_loss: 0.3790 - val_accuracy: 0.9050\n","Epoch 88/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2994 - accuracy: 0.9330 - val_loss: 0.3770 - val_accuracy: 0.9050\n","Epoch 89/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3011 - accuracy: 0.9215 - val_loss: 0.3758 - val_accuracy: 0.9050\n","Epoch 90/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.9255 - val_loss: 0.3738 - val_accuracy: 0.9050\n","Epoch 91/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.3032 - accuracy: 0.9235 - val_loss: 0.3710 - val_accuracy: 0.9050\n","Epoch 92/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2938 - accuracy: 0.9285 - val_loss: 0.3693 - val_accuracy: 0.9050\n","Epoch 93/100\n","72/72 [==============================] - 0s 6ms/step - loss: 0.2989 - accuracy: 0.9285 - val_loss: 0.3671 - val_accuracy: 0.9050\n","Epoch 94/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2864 - accuracy: 0.9365 - val_loss: 0.3650 - val_accuracy: 0.9050\n","Epoch 95/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.9305 - val_loss: 0.3633 - val_accuracy: 0.9050\n","Epoch 96/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2768 - accuracy: 0.9350 - val_loss: 0.3620 - val_accuracy: 0.9100\n","Epoch 97/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2844 - accuracy: 0.9290 - val_loss: 0.3616 - val_accuracy: 0.9050\n","Epoch 98/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2735 - accuracy: 0.9335 - val_loss: 0.3599 - val_accuracy: 0.9050\n","Epoch 99/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2779 - accuracy: 0.9315 - val_loss: 0.3589 - val_accuracy: 0.9050\n","Epoch 100/100\n","72/72 [==============================] - 0s 5ms/step - loss: 0.2577 - accuracy: 0.9440 - val_loss: 0.3574 - val_accuracy: 0.9050\n","Final model Test loss: 0.3843872845172882\n","Final model Test accuracy: 0.8914999961853027\n"]}],"source":["final_model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","final_model_history=final_model.fit(large_x_train, large_y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_validation, y_validation))\n","\n","score = final_model.evaluate(x_test, y_test, verbose=0)\n","print('Final model Test loss:', score[0])\n","print('Final model Test accuracy:', score[1])"]},{"cell_type":"markdown","metadata":{"id":"cPR3nEMAL4Yd"},"source":["(optional) Training data가 늘어나면 regularization 효과가 나는 것을 보였습니다. 쉽게 training data를 늘릴 수 있는 방법은 data augmentation 입니다. 이 방법은 기존 training data를 적절히 rotating, flipping, scaling, shifting 하여 training data 수를 늘리는 것입니다. data augmentation의 regularization 효과를 테스트해 보세요. 또한 뉴럴 네트워크의 노드 개수나 층수를 바꿔서 성능을 올려보는 것도 테스트해보세요."]},{"cell_type":"markdown","metadata":{"id":"kn11ZtM7GmS-"},"source":[]}],"metadata":{"colab":{"provenance":[]},"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"XaIWT","launcher_item_id":"zAgPl"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}